{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Caser_Parameter_Search.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNSoqMeHfsBzrfy2azuZxcU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zLbAwMQJrzOe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1597290148363,"user_tz":420,"elapsed":23146,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}},"outputId":"c83468bb-13d9-4e4a-f2c5-919407a64d2e"},"source":["from google.colab import drive\n","drive.mount('/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_aAaOrQMsBAP","colab_type":"text"},"source":["### Load Libraries "]},{"cell_type":"code","metadata":{"id":"Oq6aHo4CsAkQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290150048,"user_tz":420,"elapsed":24820,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["import pandas as pd \n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from random import choice\n","from collections import defaultdict"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8jVi0exsD9R","colab_type":"text"},"source":["### Load Training / Validation Datasets"]},{"cell_type":"code","metadata":{"id":"4FKP1c6ssIK8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290165594,"user_tz":420,"elapsed":40361,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["# Load Data\n","base_dir  = '/drive/My Drive/DSC672_Project/datasets/CASER_DATA/train/'\n","train_ds  = pd.read_csv(\n","    base_dir + 'train.csv',header=None,\n","    index_col=0,names=['uid','iid','rui','date']\n",").drop(['date','rui'],axis=1)\n","val_ds   = pd.read_csv(\n","    base_dir + 'validation.csv',header=None,\n","    index_col=0,names=['uid','iid','rui','date']\n",").drop(['date','rui'],axis=1)\n","\n","# Create User and Item Mappings to Integers\n","user_encoder = dict((uid,i) for i,uid in enumerate(train_ds.uid.unique(),start=1))\n","user_encoder['unknown'] = 0\n","user_decoder = dict((v,k) for k,v in user_encoder.items())\n","item_encoder = dict((iid,i) for i,iid in enumerate(train_ds.iid.unique(),start=1))\n","item_encoder['unknown'] = 0\n","item_decoder = dict((v,k) for k,v in item_encoder.items())\n","\n","# Create Set of all encoded Item ids\n","item_set = set(item_encoder.values())\n","\n","# Convert Raw id dataframes to encoded id dataframes\n","uid = train_ds.uid.apply(lambda x: user_encoder[x])\n","iid = train_ds.iid.apply(lambda x: item_encoder[x])\n","\n","train_ds = pd.DataFrame({'uid':uid,'iid':iid})\n","\n","uid = val_ds.uid.apply(lambda x: user_encoder[x] if x in user_encoder else user_encoder['unknown'])\n","iid = val_ds.iid.apply(lambda x: item_encoder[x] if x in item_encoder else item_encoder['unknown'])\n","\n","val_ds = pd.DataFrame({'uid':uid,'iid':iid})\n","\n","# Aggregate to sequences of climbed routes\n","# and generate sets of all routes climbed by user\n","train_ds_seq = train_ds.groupby('uid').agg([list,set])\n","val_ds_seq = val_ds.groupby('uid').agg([list,set])\n","\n","# For training data generate length of sequence for \n","# creating training sequences \n","train_ds_seq['sequence_len'] = train_ds_seq['iid','list'].apply(len)\n","\n","# For validation data create ragged tensor\n","# of routes climbed after training data\n","val_items = tf.ragged.constant(val_ds_seq['iid','list'])"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0aQpupYtXDQ","colab_type":"text"},"source":["### Function to Create Training input tensors"]},{"cell_type":"code","metadata":{"id":"oxCsDiPRtdPA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290165598,"user_tz":420,"elapsed":40362,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["def to_sequences(df,L=5,T=3,n_ratio=3):\n","  total_len = L + T \n","  n_samples = sum([c - total_len + 1 if c >= total_len else 1 for c in df['sequence_len']])\n","  \n","  n_users = df.shape[0]\n","  test_uid = np.empty((n_users,1),np.int32)\n","  test_sequence = np.empty((n_users,L),np.int32)\n","\n","  uid_input = np.empty((n_samples,1),np.int32)\n","  seq_input = np.empty((n_samples,L),np.int32)\n","  pos_targets = np.empty((n_samples,T),np.int32)\n","  neg_targets = np.empty((n_samples,T*n_ratio),np.int32)\n","  \n","  train_idx = 0\n","  test_idx = 0\n","  for breakpoint, (uid, row) in enumerate(df.iterrows()):\n","    \n","    seq, set_, _ = row\n","    seq = np.array(seq)\n","    size = len(seq)\n","    if size < total_len:\n","      pad_length = total_len - size\n","      seq = np.pad(seq,(pad_length,0),constant_values=0)\n","      size = total_len \n","\n","\n","    test_uid[test_idx] = uid\n","    test_sequence[test_idx] = seq[-L:] \n","\n","    neg_set = np.array([i for i in (item_set - set_)])\n","    for i in range(size - L - T + 1):\n","      uid_input[train_idx] = uid\n","      seq_input[train_idx] = np.array(seq[i:i+L],np.int32)\n","      pos_targets[train_idx] = np.array(seq[i+L:i+L+T],np.int32)\n","      neg_targets[train_idx] = np.random.choice(neg_set,T*n_ratio,replace=False)\n","      train_idx += 1\n","    test_idx += 1\n","  \n","  return {'training':(uid_input,seq_input,pos_targets,neg_targets),'test':(test_uid,test_sequence)}"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLmO0m3xt3bH","colab_type":"text"},"source":["### Functions to assess model performance"]},{"cell_type":"code","metadata":{"id":"Z8Lsl29pt8is","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290165599,"user_tz":420,"elapsed":40359,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["def top_k_iids(tf_model,uid,iids,k=25):\n","  # Expand to 2d Tensors\n","  uid = tf.expand_dims(uid,0)\n","  iids = tf.expand_dims(iids,0)\n","  # Predict ranking for all items \n","  predictions = tf_model.predict_all_iids(uid,iids)\n","  # Reduce Prediction to 1d Tensor\n","  predictions = tf.squeeze(predictions)\n","  # Get Top-k Items \n","  top_k = tf.argsort(predictions,direction='DESCENDING')[:k]\n","  return top_k \n","\n","def precision_and_recall(tf_model,uid,seq_iids,next_iids,k_vals=[1,5,10,25]):\n","  max_k = max(k_vals)\n","  top_max_k = top_k_iids(tf_model,uid,seq_iids,max_k).numpy()\n","\n","  precision_at_k = dict()\n","  recall_at_k = dict()\n","\n","  relevant = set(next_iids.numpy())\n","\n","  for k in k_vals:\n","\n","    top_k = set(top_max_k[:k])\n","\n","    rel_and_rec = top_k & relevant\n","\n","    precision_at_k[k] = len(rel_and_rec) / k\n","    recall_at_k[k] = len(rel_and_rec) / len(relevant)\n","  return (precision_at_k,recall_at_k)\n","\n","def validation_metric(tf_model,tf_dataset,k_vals=[1,5,10,25]):\n","  precision_at_k = defaultdict(list)\n","  recall_at_k = defaultdict(list)\n","\n","  mean_precision = defaultdict(float)\n","  mean_recall = defaultdict(float)\n","\n","  for obs in tf_dataset:\n","    uid, seq_iids, next_iids = obs\n","    precision, recall = precision_and_recall(tf_model,uid,seq_iids,next_iids,k_vals)\n","    for key in precision.keys():\n","      precision_at_k[key].append(precision[key])\n","      recall_at_k[key].append(recall[key])\n","  \n","  for key, val in precision_at_k.items():\n","    mean_precision[key] = np.array(val).mean()\n","  for key, val in recall_at_k.items():\n","    mean_recall[key] = np.array(val).mean()\n","\n","  return mean_precision, mean_recall"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQ_J9_g5uJd_","colab_type":"text"},"source":["### Caser Model "]},{"cell_type":"code","metadata":{"id":"BfaVUB-jy7to","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290166093,"user_tz":420,"elapsed":40848,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["class TFCaser(tf.Module):\n","  def __init__(self,n_uid,n_iid,L,\n","               embedding=8,n_v=16,n_h=16,\n","               dropout_rate=0.5,l2_lambda=0.001,\n","               conv_act=tf.nn.relu,\n","               dense_act=tf.nn.relu,\n","               initializer = tf.initializers.GlorotNormal\n","    ):\n","    super(TFCaser,self).__init__(name='Caser')\n","    # Item and User Embedding\n","    self.n_uid = n_uid\n","    self.n_iid = n_iid\n","    self.embedding = embedding\n","    self.initalizer = initializer()\n","\n","\n","    self.U = tf.Variable(\n","        self.initalizer(shape=(self.n_uid,self.embedding))\n","    )\n","    self.I = tf.Variable(\n","        self.initalizer((self.n_iid,self.embedding))\n","    )\n","\n","    # Sequence Length\n","    self.L = tf.Variable(tf.constant(L),trainable=False)\n","    self.int_L = L\n","\n","    # Vertical and Horizontal Convolutions\n","    self.n_v = n_v\n","    self.v_conv = tf.Variable(self.initalizer((self.L,1,1,self.n_v)))\n","    self.v_conv_bias = tf.Variable(tf.zeros(self.n_v))\n","    self.n_h = n_h\n","    self.lengths = [i for i in range(1,L+1)]\n","    self.h_conv  = [\n","        tf.Variable(self.initalizer((l,self.embedding,1,self.n_h)))\n","        for l in self.lengths\n","    ]\n","    self.h_conv_bias = [\n","        tf.Variable(tf.zeros(self.n_h))\n","        for _ in self.lengths\n","    ]\n","    # Convolution Activation\n","    self.conv_act = conv_act\n","\n","    # Dense Layer\n","    ## Input Size\n","    self.dense_v = self.embedding * self.n_v\n","    self.dense_h = self.n_h * len(self.lengths)\n","    self.dense_in = self.dense_v + self.dense_h\n","    ## Activation\n","    self.dense_act = dense_act\n","    ## Weights and Biases\n","    self.W_1 = tf.Variable(self.initalizer((self.dense_in,self.embedding)))\n","    self.B_1 = tf.Variable(tf.zeros(self.embedding))\n","\n","    # Output Layer\n","    self.W_2 = tf.Variable(self.initalizer((self.n_iid,self.embedding * 2)))\n","    self.B_2 = tf.Variable(tf.zeros(self.n_iid))\n","\n","    # Regularization\n","    self.dropout_rate = dropout_rate\n","    self.l2_lambda = l2_lambda\n","\n","    # Weight Tensors for L2 Regularization\n","    self.weights = [\n","        self.U,self.I,\n","        self.W_1,self.W_2,\n","        self.v_conv\n","    ] + self.h_conv\n","\n","\n","\n","  def forward_to_last(self,uid,iids,training):\n","    u_emb = tf.squeeze(tf.nn.embedding_lookup(self.U,uid),1)\n","\n","    s_emb = tf.nn.embedding_lookup(self.I,iids)\n","    s_emb = tf.expand_dims(s_emb,-1)\n","\n","    # Vertical Convolutions\n","    v = self.conv_act(tf.nn.conv2d(s_emb,self.v_conv,1,'VALID'))\n","    v = tf.nn.bias_add(v,self.v_conv_bias)\n","    v = tf.reshape(v,(-1,self.dense_v))\n","\n","    # Horizontal Convolutions\n","    out_hs = tf.TensorArray(tf.float32,self.L)\n","\n","    for idx, (conv, conv_bias) in enumerate(zip(self.h_conv,self.h_conv_bias)):\n","      h = self.conv_act(tf.nn.conv2d(s_emb,conv,1,'VALID'))\n","      h = tf.nn.bias_add(h,conv_bias)\n","      h = tf.squeeze(h,2)\n","      h = tf.nn.max_pool1d(h,h.shape[1],1,'VALID')\n","      out_hs = out_hs.write(idx,tf.transpose(h))\n","    h = out_hs.concat()\n","    h = tf.transpose(h)\n","    h = tf.squeeze(h,1)\n","\n","    # Concat Vertical and Horizontal Convolutional Outputs\n","    dense_in = tf.concat([v,h],1)\n","    # Apply Dense Layer\n","    dense_out = tf.add(tf.matmul(dense_in,self.W_1),self.B_1)\n","    dense_out = self.dense_act(dense_out)\n","\n","    dense_out = tf.cond(\n","        training,\n","        lambda: tf.nn.dropout(dense_out,self.dropout_rate),\n","        lambda: dense_out\n","    )\n","    #if training:\n","      # If training model apply dropout\n","      #dense_out = tf.nn.dropout(dense_out,self.dropout_rate)\n","    \n","    # Concat user embedding with dense layer\n","    x = tf.concat([u_emb,dense_out],1)\n","    return x\n","\n","\n","  def predict_with_iids(self,uid,seq_iids,target_iids,training):\n","    x = self.forward_to_last(uid,seq_iids,training)\n","    w2 = tf.nn.embedding_lookup(self.W_2,target_iids)\n","    b2 = tf.nn.embedding_lookup(self.B_2,target_iids)\n","    x = tf.expand_dims(x,1)\n","    res = tf.reduce_sum(x*w2,-1) + b2\n","    return res\n","\n","\n","  def predict_all_iids(self,uid,seq_iids):\n","    x = self.forward_to_last(uid,seq_iids,tf.constant(False))\n","    res = tf.add(tf.matmul(x,self.W_2,transpose_b=True),self.B_2)\n","    return res\n","\n","  def build(self):\n","    # Build TF Functions\n","    self.forward_to_last = tf.function(\n","      self.forward_to_last,\n","      input_signature=[\n","          tf.TensorSpec((None,1),tf.int32,name='User ID'),\n","          tf.TensorSpec((None,self.L.numpy()),tf.int32,name='Item Sequence'),\n","          tf.TensorSpec((),tf.bool,name='Training Switch')\n","      ]\n","    )\n","\n","    self.predict_with_iids = tf.function(\n","        self.predict_with_iids,\n","        input_signature=[\n","          tf.TensorSpec((None,1),tf.int32,name='User ID'),\n","          tf.TensorSpec((None,self.L.numpy()),tf.int32,name='Item Sequence'),\n","          tf.TensorSpec((None,None),tf.int32,name='Prediction Items'),\n","          tf.TensorSpec((),tf.bool,name='Training Switch')\n","      ]\n","    )\n","\n","    self.predict_all_iids = tf.function(\n","        self.predict_all_iids,\n","        input_signature=[\n","          tf.TensorSpec((None,1),tf.int32,name='User ID'),\n","          tf.TensorSpec((None,self.L.numpy()),tf.int32,name='Item Sequence')\n","      ]\n","\n","    )"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CG8dQUiuwmyE","colab_type":"text"},"source":["### Caser Training Function"]},{"cell_type":"code","metadata":{"id":"3etuypt-wueM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290166094,"user_tz":420,"elapsed":40846,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["def train_caser(caser_model,training_dataset,\n","                n_epochs,optimizer,batch_size,\n","                steps_per_epoch,\n","                validation_dataset=None,\n","                val_monitor=None):\n","  if val_monitor == None:\n","    val_monitor = n_epochs\n","\n","  training_dataset = training_dataset.shuffle(1000)\n","  training_dataset = training_dataset.batch(batch_size=batch_size)\n","\n","\n","  history = {'train_loss':[],'precision':[],'recall':[]}\n","  for e in range(n_epochs):\n","    epoch_loss = 0\n","    for bdx, batch in enumerate(training_dataset,start=1):\n","      uid, seq, y_poss, y_negs = batch\n","      \n","      with tf.GradientTape() as tape:\n","        pos_pred = caser_model.predict_with_iids(uid, seq, y_poss,training=tf.constant(True))\n","        neg_pred = caser_model.predict_with_iids(uid, seq, y_negs,training=tf.constant(True))\n","        pos_loss = - tf.reduce_mean(tf.math.log(tf.sigmoid(pos_pred + 1e-10)))\n","        neg_loss = - tf.reduce_mean(tf.math.log((1-tf.sigmoid(neg_pred))+1e-10))\n","        total_loss = pos_loss + neg_loss\n","        l2 = sum([tf.nn.l2_loss(t).numpy() for t in caser_model.weights])\n","        reg_loss = (l2 * caser_model.l2_lambda) + total_loss\n","      params= caser_model.trainable_variables\n","      grads = tape.gradient(reg_loss,params)\n","      optimizer.apply_gradients(zip(grads,params))\n","      epoch_loss = epoch_loss + total_loss\n","      if bdx == steps_per_epoch: break\n","\n","    loss = epoch_loss/(bdx) \n","    history['train_loss'].append(loss.numpy())\n","    print(f'Epoch {e} train loss: {loss.numpy():0.4f}')\n","\n","    if (e + 1) % val_monitor == 0 and validation_dataset:\n","      precision, recall = validation_metric(caser_model,validation_dataset)\n","      history['recall'].append(recall)\n","      history['precision'].append(precision)\n","  \n","  return history"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZEqBMrSxAVN","colab_type":"text"},"source":["### Random Search"]},{"cell_type":"markdown","metadata":{"id":"2ZkX39JFE0IB","colab_type":"text"},"source":["Building Training Sequences is slow so for sets of L and T perform random search for each set"]},{"cell_type":"markdown","metadata":{"id":"Wd6kHItmEvlP","colab_type":"text"},"source":["#### Caser L - T\n"]},{"cell_type":"code","metadata":{"id":"yIGmcw5aw_1L","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597290657131,"user_tz":420,"elapsed":531881,"user":{"displayName":"Ross Pollock","photoUrl":"","userId":"15804473961280684603"}}},"source":["# Set Up Experiment Search Space\n","\n","LOG_FILE = '/drive/My Drive/DSC672_Project/model_results/CASER/random_search.csv'\n","\n","N_USERS = len(user_encoder)\n","N_ITEMS = len(item_encoder)\n","\n","SEQ_LENGTH = 9\n","T = 1\n","L2_REG = 1e-4\n","N_EPOCHS = 10\n","OPTIMIZER = tf.optimizers.Adam(1e-3)\n","BATCH_SIZE = 256\n","\n","EMBEDDING_DIMS = [2**i for i in range(2,10)]\n","NUM_FILTERS = [2**i for i in range(1,7)]\n","ACTIVATIONS = {\n","    'ReLU':tf.nn.relu,\n","    'Sigmoid':tf.nn.sigmoid,\n","    'Tanh':tf.nn.tanh,\n","    'Linear':tf.keras.activations.linear\n","}\n","\n","\n","NUMBER_OF_EXPERIMENTS = 3\n","\n","\n","seq_data = to_sequences(train_ds_seq,SEQ_LENGTH,T,n_ratio=5)\n","uids,iids,y_pos,y_neg = seq_data['training']\n","val_uids,val_seq = seq_data['test']\n","val_uids = val_uids[val_ds_seq.index.to_numpy() - 1]\n","val_seq = val_seq[val_ds_seq.index.to_numpy() - 1]\n","\n","training_dataset = tf.data.Dataset.from_tensor_slices((uids,iids,y_pos,y_neg))\n","validation_dataset = tf.data.Dataset.from_tensor_slices((val_uids,val_seq,val_items))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Zl4TTlZ5f2M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"outputId":"bf1ee731-d1d8-4706-d05d-a5e71e66cf2b"},"source":["# Random Search Loop\n","for _ in range(NUMBER_OF_EXPERIMENTS):\n","  emb_dim = choice(EMBEDDING_DIMS)\n","  v_filters = choice(NUM_FILTERS)\n","  h_filters = choice(NUM_FILTERS)\n","\n","  conv_act_name, conv_act = choice(list(ACTIVATIONS.items()))\n","  dense_act_name, dense_act = choice(list(ACTIVATIONS.items()))\n","\n","  # Model Hyper Parameters\n","  model_hyper_params = [\n","    SEQ_LENGTH,T,\n","    emb_dim,v_filters,h_filters,\n","    conv_act_name,dense_act_name  \n","  ]\n","\n","  # Create Model\n","  model = TFCaser(\n","      N_USERS, N_ITEMS, L=SEQ_LENGTH,\n","      embedding=emb_dim,n_v=v_filters,n_h=h_filters,l2_lambda=L2_REG,\n","      conv_act = conv_act, dense_act=dense_act\n","  )\n","\n","  model.build()\n","\n","  # Train Model\n","  model_history = train_caser(\n","      model,training_dataset,\n","      N_EPOCHS,OPTIMIZER,BATCH_SIZE,\n","      uids.shape[0] // BATCH_SIZE,\n","      validation_dataset\n","  )\n","\n","  # Validation Metrics\n","  precision = list(model_history['precision'][-1].values())\n","  recall = list(model_history['recall'][-1].values())\n","\n","  # Append Results and Hyperparameters to Logging File\n","  log_line = model_hyper_params + precision + recall \n","  log_line = ','.join(map(str,log_line))\n","  with open(LOG_FILE,'a') as f:\n","    f.write(log_line + '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0 train loss: 1.0624\n","Epoch 1 train loss: 0.8451\n","Epoch 2 train loss: 0.6878\n","Epoch 3 train loss: 0.5802\n","Epoch 4 train loss: 0.5094\n","Epoch 5 train loss: 0.4578\n","Epoch 6 train loss: 0.4175\n","Epoch 7 train loss: 0.3842\n","Epoch 8 train loss: 0.3558\n","Epoch 9 train loss: 0.3316\n","Epoch 0 train loss: 1.2185\n","Epoch 1 train loss: 1.0772\n","Epoch 2 train loss: 0.9892\n","Epoch 3 train loss: 0.9190\n","Epoch 4 train loss: 0.8635\n","Epoch 5 train loss: 0.8176\n","Epoch 6 train loss: 0.7791\n","Epoch 7 train loss: 0.7455\n","Epoch 8 train loss: 0.7169\n","Epoch 9 train loss: 0.6921\n","Epoch 0 train loss: 1.1565\n","Epoch 1 train loss: 1.0134\n","Epoch 2 train loss: 0.9242\n","Epoch 3 train loss: 0.8528\n","Epoch 4 train loss: 0.7993\n","Epoch 5 train loss: 0.7577\n","Epoch 6 train loss: 0.7252\n","Epoch 7 train loss: 0.6978\n","Epoch 8 train loss: 0.6741\n","Epoch 9 train loss: 0.6528\n","Epoch 0 train loss: 1.5471\n","Epoch 1 train loss: 0.8600\n","Epoch 2 train loss: 0.5756\n"],"name":"stdout"}]}]}
